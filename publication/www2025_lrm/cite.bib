@inproceedings{10.1145/3701716.3715583,
author = {Yang, Mingdai and Liu, Zhiwei and Yang, Liangwei and Liu, Xiaolong and Wang, Chen and Peng, Hao and Yu, Philip S.},
title = {Training Large Recommendation Models via Graph-Language Tokens Alignment},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715583},
doi = {10.1145/3701716.3715583},
abstract = {Recommender systems (RS) have become essential tools for helping users efficiently navigate the overwhelming amount of information on e-commerce and social platforms. However, traditional RS relying on Collaborative Filtering (CF) struggles to integrate the rich semantic information from textual data. Meanwhile, large language models (LLMs) have shown promising results in natural language processing, but directly using LLMs for recommendation introduces challenges, such as ambiguity in generating item predictions and inefficiencies in scalability. In this paper, we propose a novel framework to train Large Recommendation models via Graph-Language Token Alignment. By aligning item and user nodes from the interaction graph with pretrained LLM tokens, GLTA effectively leverages the reasoning abilities of LLMs. Furthermore, we introduce Graph-Language Logits Matching (GLLM) to optimize token alignment for end-to-end item prediction, eliminating ambiguity in the free-form text as recommendation results. Extensive experiments on three benchmark datasets demonstrate the effectiveness of GLTA, with ablation studies validating each component.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1470â€“1474},
numpages = {5},
keywords = {large language models, recommender system},
location = {Sydney NSW, Australia},
series = {WWW '25}
}