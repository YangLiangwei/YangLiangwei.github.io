@inproceedings{10.1145/3705328.3748036,
author = {Zhang, Weizhi and Yang, Liangwei and Song, Zihe and Zou, Henry Peng and Xu, Ke and Zhu, Yuanjie and Yu, Philip S.},
title = {SGCL: Unifying Self-Supervised and Supervised Learning for Graph Recommendation},
year = {2025},
isbn = {9798400713644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705328.3748036},
doi = {10.1145/3705328.3748036},
abstract = {Recommender systems (RecSys) are essential for online platforms, providing personalized suggestions to users within a vast sea of information. Self-supervised graph learning seeks to harness high-order collaborative filtering signals through unsupervised augmentation on the user-item bipartite graph, primarily leveraging a multi-task learning framework that includes both supervised recommendation loss and self-supervised contrastive loss. However, this separate design introduces additional graph convolution processes and creates inconsistencies in gradient directions due to disparate losses, resulting in prolonged training times and sub-optimal performance. In this study, we introduce a unified framework of Supervised Graph Contrastive Learning for recommendation (SGCL) to address these issues. SGCL uniquely combines the training of recommendation and unsupervised contrastive losses into a cohesive supervised contrastive learning loss, aligning both tasks within a single optimization direction for exceptionally fast training. Extensive experiments on three real-world datasets show that SGCL outperforms state-of-the-art methods, achieving superior accuracy and efficiency.},
booktitle = {Proceedings of the Nineteenth ACM Conference on Recommender Systems},
pages = {671â€“676},
numpages = {6},
keywords = {Graph Recommendation, Self-Supervised Learning, Contrastive Learning},
location = {
},
series = {RecSys '25}
}