@inproceedings{liu-etal-2024-pract,
    title = "{PRACT}: Optimizing Principled Reasoning and Acting of {LLM} Agent",
    author = "Liu, Zhiwei  and
      Yao, Weiran  and
      Zhang, Jianguo  and
      Murthy, Rithesh  and
      Yang, Liangwei  and
      Liu, Zuxin  and
      Lan, Tian  and
      Zhu, Ming  and
      Tan, Juntao  and
      Kokane, Shirley  and
      Hoang, Thai  and
      Niebles, Juan Carlos  and
      Heinecke, Shelby  and
      Wang, Huan  and
      Savarese, Silvio  and
      Xiong, Caiming",
    editor = "Barak, Libby  and
      Alikhani, Malihe",
    booktitle = "Proceedings of the 28th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.conll-1.33/",
    doi = "10.18653/v1/2024.conll-1.33",
    pages = "442--446",
    abstract = "We introduce the Principled Reasoning and Acting (PRAct) framework, a novel method for learning and enforcing action principles from trajectory data. Central to our approach is the use of text gradients from a reflection and optimization engine to derive these action principles. To adapt action principles to specific task requirements, we propose a new optimization framework, Reflective Principle Optimization (RPO). After execution, RPO employs a reflector to critique current action principles and an optimizer to update them accordingly.We investigate the RPO framework under two scenarios: Reward-RPO, which uses environmental rewards for reflection, and Self-RPO, which conducts self-reflection without external rewards. Additionally, we developed two RPO methods, RPO-Traj and RPO-Batch, to adapt to different settings.Experimental results across four environments demonstrate that the PRAct agent, leveraging the RPO framework, can effectively learn and apply action principles to enhance performance."
}